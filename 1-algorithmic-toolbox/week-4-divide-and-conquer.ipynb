{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Intro](https://www.coursera.org/learn/algorithmic-toolbox/lecture/z0rJZ/intro)\n",
    "Divide and conquer comprises:\n",
    "1. breaking the problem into non-overlapping subproblems of the same type (you cannot make one rectangle and another triangle). \n",
    "2. recursively solving those subproblems.\n",
    "3. combining the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear search\n",
    "- You search an element in an array. You just start from the beginning and try to find it as you go.\n",
    "- If `len(array)` is too long, it'd take too long as well. \n",
    "- Iterate through the array until you find the chosen element. If you reach the end of the array and haven't yet found the element, return NOT_FOUND.\n",
    "- Divide and conquer for linear search: `LinearSearch(Arr, lowBoundary, highBoundary, keyToSearch)`\n",
    "    for each iteration, check if `Arr[lowBoundary]` = `keyToSearch` and if this is false, do another call with `LinearSearch(Arr, lowBoundary+1, highBoundary, keyToSearch)`. Do this until `lowBoundary > highBoundary`.\n",
    "    This is a very subtle divide and conquer algo.\n",
    "\n",
    "## Recurrence relation\n",
    "- is an equation **recursively** defining a sequence of values\n",
    "- Just like fibonacci\n",
    "- Runtime of linear serach would make it `c * n` where `c` is the constant amount of work done each iteration (recursion)\n",
    "\n",
    "# Binary search\n",
    "- Searching sorted data (like a real dictionary). Imagine \n",
    "\n",
    "## Problem statement\n",
    "- `A[i]` is no more than `A[i+1]` but could be the same. `A` is an array. There could be however duplicates in an array. \n",
    "- This is called monotonic non-decreasing array.\n",
    "- If you cannot find the key in the array, which index would it have been in?\n",
    "\n",
    "## Searching in a sorted array\n",
    "- Easy. Just return the index where the number would have been in in a sorted array.\n",
    "\n",
    "## Impl (in a sorted array)\n",
    "- `BinarySearch(Array, lowBoundary, highBoundary, keyToSearch)\n",
    "- You wanna find out the **midpoint**. do `high-low` and divide by `2`. Floor it so that it is not going to be a fraction.\n",
    "- If you find `keyToSearch` equal to `midpoint`, return `midpoint` (the index). You are done.\n",
    "- Else, because it is a sorted array, `if key < A[midpoint]`. recursively call `BinarySearch(Array, lowBoundary, mid - 1, keyToSearch)`\n",
    "- If it's bigger, just change that to `mid + 1`.\n",
    "- What's good here is that you **can ignore the half of the array each time**. \n",
    "\n",
    "> We've recursively solved the subproblems. And then we're going to combine the results of those subproblems. We broke the problem into a problem of size half (slightly less than half). We recursively solved that single subproblem and then we combined the result very simply just by returning the result.\n",
    "\n",
    "# Binary search runtime\n",
    "- recall the binary search algo\n",
    "- recurrence relation for the worst runtime: you don't find the element. \n",
    "- Runtime goes like this:\n",
    "```\n",
    "n\n",
    "n/2\n",
    "n/4\n",
    "...\n",
    "2\n",
    "1\n",
    "0\n",
    "```\n",
    "- Seeing top-down, you get `n` broken down by half repeatedly. If you are doing so, it is going to take `log base 2` until you get to 1. \n",
    "- at each level, you are doing a constant amount of work `c`\n",
    "- You can ignore the base because it's anyways a constant work. \n",
    "- You could also have `while` version in contrast to the recursive version. It's essentially the same. \n",
    "\n",
    "## Real life example\n",
    "- Augmented set of arrays\n",
    "- Use pointers in another array to remember corresponding words\n",
    "- Compare these pointer-containing arrays to find match\n",
    "- Use this method: of course you've got a space-time trade off. But if there were 5000 words, instead of searching for the time of `T(50000)`, you could search in `T(log(50000))` which is about `T(15.x)`. So instead of 50000 references you should make, you can really reduce it down to 15 times.  \n",
    "\n",
    "## Summary\n",
    "- The runtime of binary search is big theta of `log(n)`. (For reviewing big theta, see [this stackoverflow post](https://stackoverflow.com/questions/49880681/time-complexity-understanding-big-theta)). In short, the theta means the worst and the best cases are run in the same time complexity always.\n",
    "\n",
    "# Polynomial multiplications\n",
    "- Usages: multiplying large integers together..\n",
    "\n",
    "## Multiplying polynomials\n",
    "- You only need coefficients\n",
    "- Just middle school math\n",
    "\n",
    "## Naive algo\n",
    "- Just calculate each case one by one in a nested for loop\n",
    "\n",
    "## Naive divide and conquer algo\n",
    "- Divide the expression into high and lower terms (the upper half and the lower half)\n",
    "\n",
    "![18](./files/18.PNG)\n",
    "\n",
    "- We just need to calculate D1E1, D1E0, D0E1, and D0E0 \n",
    "- **Recurrence relation: $T(n) = 4T(n/2) + kn$. 4 because you are breaking the problem into 4 subproblems: the problem is broken in half and there are two problems here (which means 2 * 2) and the size of the problem is only half, so that's for $n/2$. $kn$ stands for the constant amount of work done each time** \n",
    "- **n**: number of variables? in the expression. in the example below: 4\n",
    "- **notice that this is just another way of writing a normal polynomial.** look at how `a(n-1), a(n-2), ... move towards the half: a(n/2) and then lower: a(n/2 - 1) and so on.`\n",
    "\n",
    "### Example\n",
    "![19](./files/19.PNG)\n",
    "\n",
    "Example getting D1(X), where `n` is 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$4x^{3}+3x^{2}$$\n",
    "$$=a_{3}x^{\\frac{4}{2}-1}+a_{2}$$\n",
    "$$=a_{3}x+a_{2}$$\n",
    "$$= 4x+3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if `n` is not a power of 2 minus 1\n",
    "- The current equation only seems to solve problems where `n = x^2 - 1` where `x` is some integer, because there is `n/2` in the equation everywhere. \n",
    "- What do we do? We can instead insert `0` for all those polynomials that contain nothing, while still preserving the power as it was.\n",
    "- For example:\n",
    "$$\n",
    "A(x)=5x^{6}+3x^{5}+4x^{2}+1\n",
    "$$\n",
    "$$\n",
    "B(x)=6x^{5}+5\n",
    "$$\n",
    "\n",
    "You are going to set `n` as `8` instead of `7` because \n",
    "$$\n",
    "x^{2} > 7\n",
    "$$\n",
    "The smallest integer possible for x is \n",
    "$$\n",
    "3\n",
    "$$\n",
    "which makes \n",
    "$$\n",
    "x^{2} = 8\n",
    "$$\n",
    "\n",
    "So really, into the equation, you are going to insert:\n",
    "```\n",
    "A = [0, 5, 3, 0, 0, 4, 0, 1]\n",
    "\n",
    "B = [0, 0, 6, 0, 0, 0, 0, 5]\n",
    "\n",
    "n = 8\n",
    "```\n",
    "\n",
    "### Time complexity\n",
    "you divide the equation into quadruple, and then quadruple of quadruple, and so on. Then you are going to have $4^{i}$ problems at level $i$.\n",
    "\n",
    "\n",
    "![20](./files/20.PNG)\n",
    "\n",
    "#### Level \n",
    "\n",
    "Remember that `n` is always to the power of 2, so the result will always make an integer. \n",
    "\n",
    "For example `n = 4`\n",
    "```\n",
    "4\n",
    "2 2 2 2 \n",
    "1111 1111 1111 1111\n",
    "```\n",
    "\n",
    "because the level increases 1 by 1 (1,2,3,4,5 ...) but `n` increases by the power of 2, that makes the level at each `n`: $log_2{n}$. For example if $n=4$ level's only `2` because you need 4, 2, 1 at each level, so it totals `2` ($log_2{4} = 2$).\n",
    "\n",
    "#### Number of problems\n",
    "The number of problems grow exponentially, with the base of `4`. And then you got $log_2{n}$ levels in total. Then that makes $4^{log_2{n}}$ for a certain $n$.\n",
    "\n",
    "#### Number of works\n",
    "$works = numOfProblems * amountOfWorkAtEachLevel$\n",
    "- at the first level, you got `kn` because you only work once. \n",
    "- Next, you got $4(k)(n/2)$ because there 4 works to do but these works only cost $n/2$ work. $k$ is still a constant amount of work done each time.\n",
    "![21](./files/21.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So really, the time complexity of the amount of works is the total of all the works.\n",
    "- It's **theta of n^2**, because that last term at the bottom (`kn^2`) dominates over any other terms added up. \n",
    "\n",
    "#### Weird\n",
    "This runtime is the same as the naive algo we viewed above. What could be a better solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster divide and conquer algo\n",
    "## Karatsuba approach\n",
    "- normal expansion of multiplication of two polynomials of degree one. It takes 4 multiplications. Simple.\n",
    "- wow. but if you change that into... something else, **you only have to do 3 multiplications in total.**\n",
    "![22](./files/22.PNG)\n",
    "- you are reducing the number of problems at each level.\n",
    "- let's look at the same problem we looked at last time. It only takes 3 multiplications now this way\n",
    "![23](./files/23.PNG)\n",
    "\n",
    "## Runtime \n",
    "- You still have **same levels** because `n` has not been affected by this approach\n",
    "- The **number of problems** you got increases to the power of `3`. Then you will have $3^{log_2{n}}$\n",
    "- The **number of works** is just `kn` times the number of problems you got each level.\n",
    "![24](./files/24.PNG)\n",
    "\n",
    "- Of course $log_2{3}$ is the highest order of polynomial in the summation. So it dominates. You can ignore other terms. \n",
    "\n",
    "![25](./files/25.PNG)\n",
    "\n",
    "## Gotchas\n",
    "- You sometimes need a different way of looking at the problem \n",
    "- This will hopefully reduce the number of problems you need to solve at each level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplying big numbers with the polynomial multiplication method\n",
    "- We can apply polynomial multiplication algo to compute multiplication of big integers. \n",
    "- For integer $a_n$ inside a number (where n is the length of digits of number), put it as $a_nx^{n}$\n",
    "- So it will become like \n",
    "$a(x)=a_1x^{n−1}+a_2x^{n−2}+⋯+a_n$\n",
    "- For example,\n",
    "$123$ would be $1x^{3-1}+2x^{3-2}+3 = x^{2}+2x+3$\n",
    "and you know if $x=10$\n",
    "This makes the original number because\n",
    "$10^{2}+2(10)+3 = 100 + 20 + 3$\n",
    "- Then you multiply those two numbers together. Apply Karatusba's algo. \n",
    "- Get the resulting polynomial. If some coefficients are bigger than 9, push that to left coefficient because it can contain them. For example,\n",
    "$$1\\cdot10^{2} + 0\\cdot10^{1} + 23$$ \n",
    "is obviously 123. But you can push 20 to the left coefficient:\n",
    "$$1\\cdot10^{2} + 2\\cdot10^{1} + 3$$ \n",
    "Now the coefficients, listed from left to right, make the desired number $123$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master theorem\n",
    "- Rather than creating a recurrence tree each time to get time complexity, you just want to have a formulae.\n",
    "\n",
    "![26](./files/26.PNG)\n",
    "\n",
    "- You got a ceiling and it could be a floor as well or not be there at all if $n$ were a power of $b$, not $d$. \n",
    "- All three recurrence cases depend on the relationship of $a$, $b$, and $d$.\n",
    "- **This theorem allows us to simply calculate time complexity for most recurrences happening in divide & conquer problems.** \n",
    "\n",
    "# Proof of master theorem\n",
    "![27](./files/27.PNG)\n",
    "- You have got a typical recurrence here. \n",
    "$$\n",
    "T(n) = aT(Ceiling({\\frac{n}{b}})) + O(n^{d})\n",
    "$$\n",
    "- assume $n$ is a power of $b$. We can always add a pad to make it larger. \n",
    "- you got a problem $n$. \n",
    "- you will have $a$ times copies of problems as you get down to the bottom. At the very bottom, you are going to have $a^{log_b{n}}$ number of problems because:\n",
    "- you are going to have up to level $log_b{n}$ where you hit $1$s at the bottom.\n",
    "- work is just a function of how many problems we have and the amount of work for each problem.\n",
    "    - calculating work for each level: you can pull out $b$ and $a$ because they are just constants. What's important is $n$.\n",
    "\n",
    "## Geometric series\n",
    "![28](./files/28.PNG)\n",
    "- Easy. Just account for the largest term for each case. \n",
    "\n",
    "## Geometric series in summation from recurrence trees\n",
    "- Go back to the summation equation derived from the master theorem. \n",
    "- $r$ in geometric series formulae is just $\\frac{a}{b^{d}}$ here.\n",
    "There are 3 cases in the master theorem, and we are going to explain this with this:\n",
    "\n",
    "### Case 1\n",
    "If $a < b^{d}$, as you reach the last term, the terms would get smaller and smaller. Then that only leaves $O(n^{d})$, the largest term, which is also the first term.\n",
    "\n",
    "![29](./files/29.PNG)\n",
    "\n",
    "### Case 2\n",
    "$a= b^{d}$, which means $a/b^{d} = 1$\n",
    "\n",
    "![30](./files/30.PNG)\n",
    "\n",
    "The result is just `number of terms in summation times O(n^d)`. Well you've got `1` because you start from $i=0$.\n",
    "\n",
    "And then you can ignore 1 beacuse it is a low order term. You also don't care about the base. Then you get \n",
    "$$\n",
    "O(n^{d}logn)\n",
    "$$\n",
    "\n",
    "### Case 3\n",
    "- $a > b^{d}$. Then obviously the **largest term** would be the **last term.**\n",
    "- So just insert the last term. Then expand, and solve. Use logarithmic rules. \n",
    "\n",
    "![31](./files/31.PNG)\n",
    "\n",
    "## Prof's tip\n",
    "- If you have a recurrence tree, just look at the first and second work. \n",
    "- If it's increasing as you go, it's case 3. If amount of work is the same all the way, it's case 2. And you know what's for case 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
